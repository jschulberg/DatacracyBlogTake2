---
title: Next Word Generation with Markov Chains
author: Justin S.
date: '2024-01-15'
slug: [Markov-Chain]
categories:
  - nlp
  - markov
tags:
  - Python
comments: yes
featured_image: ''
share: yes
draft: true
description: '<Insert description here>'
---



<p>This article leverages some work I did for my grad program’s Bayesian Statistics Course. Specifically, it uses Bayesian inference for the task of <a href="https://arxiv.org/abs/2306.17184"><em>next word prediction</em></a>.</p>
<div id="next-word-prediction" class="section level1">
<h1>Next Word Prediction</h1>
<p>A common task that language models try to solve is next word prediction. Given a series of words in a sentence, can the language model predict the <em>next</em> word that should appear? Ideally, the model would use understanding of the prior words that appeared</p>
<div id="bayes-formula" class="section level2">
<h2>Bayes’ Formula</h2>
<p>The traditional Bayes’ formula, given events A and B, is:</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A) * P(A)}{P(B)}
\]</span></p>
<p>Given a prior, <span class="math inline">\(\theta\)</span>, which represents the probability distribution of a data object <em>before</em> it’s observed, and a likelihood <span class="math inline">\(y\)</span>, which represents the probability of falling under a specific category/class, we can generalize the above formula to compute the posterior as such:</p>
<p><span class="math display">\[
p(\theta|y) = \frac{p(y|\theta)*p(\theta)}{p(y)}
\]</span></p>
<p>where <span class="math inline">\(p(\theta|y)\)</span> represents the posterior probability: the probability after the evidence from our prior is considered.</p>
</div>
<div id="a-simple-application-of-bayes-for-next-word-prediction" class="section level2">
<h2>A Simple Application of Bayes for Next Word Prediction</h2>
<p>Let’s start simple. We’ll take some text and tokenize it into words. I’ll then create a matrix that shows the probability that a word appears given the word that comes directly before it. Again, this is a naive approach because I will <em>only</em> look at the word that comes immediately before. That makes the approach act more like a <a href="https://www.jstor.org/stable/2584334">Markovian transition matrix</a>, since it’s just looking at the state immediately before to make a prediction, rather than all the events that led up to the prediction.</p>
<div class="float">
<img src="images/markov.png" alt="markov" />
<div class="figcaption">markov</div>
</div>
<p>Markov chain analysis is a mathematical framework used to model and analyze systems that undergo transitions between different states over time. A Markov chain is a stochastic process where the future state depends only on the current state and not on the sequence of events that preceded it. In this case our “state” is the current word being observed. We will use the current state (word) to predict the next state (word).</p>
<pre><code>## 
## I am so excited to graduate from Georgia Tech! 
## I have loved my time studying at Georgia Tech and cannot wait to graduate.</code></pre>
</div>
<div id="word-bi-gram-co-occurrence-matrix" class="section level2">
<h2>Word Bi-gram Co-occurrence Matrix</h2>
<p>To start, I’ll tokenize the text into words and split it into bigrams. Using the bigrams, I’ll create a co-occurence matrix, <span class="math inline">\(P_{ij}\)</span> where <span class="math inline">\(i\)</span> represents the <span class="math inline">\(i^{\text{th}}\)</span> word and <span class="math inline">\(j\)</span> represents the <span class="math inline">\(j^{\text{th}}\)</span> word. The value of <span class="math inline">\(P_{ij}\)</span> then is the probability that word <span class="math inline">\(j\)</span> occurs immediately after word <span class="math inline">\(i\)</span>.</p>
<pre><code>##             i   am   so  excited   to  ...   at  and  can  not  wait
## i         0.0  0.5  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## am        0.0  0.0  1.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## so        0.0  0.0  0.0      1.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## excited   0.0  0.0  0.0      0.0  1.0  ...  0.0  0.0  0.0  0.0   0.0
## to        0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## graduate  0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## from      0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## georgia   0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## tech      0.5  0.0  0.0      0.0  0.0  ...  0.0  0.5  0.0  0.0   0.0
## have      0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## loved     0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## my        0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## time      0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## studying  0.0  0.0  0.0      0.0  0.0  ...  1.0  0.0  0.0  0.0   0.0
## at        0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   0.0
## and       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  1.0  0.0   0.0
## can       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  1.0   0.0
## not       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0   1.0
## wait      0.0  0.0  0.0      0.0  1.0  ...  0.0  0.0  0.0  0.0   0.0
## 
## [19 rows x 19 columns]</code></pre>
<p>In the Markov transition matrix above we can see how, in the first row, given the word <code>I</code> (our current state), there are two words that could appear (our next state):</p>
<p><span class="math display">\[
P(\text{am } | \text{ I}) = .5 \\
P(\text{have } | \text{ I}) = .5
\]</span></p>
<p>It’s important to note that these probabilities will <strong>always</strong> add up to 1; an important property of a <a href="https://acme.byu.edu/00000179-af25-d5e1-a97b-bf6512fd0000/markov2020-pdf#:~:text=Thus%2C%20each%20of%20the%20columns,transition%20matrix%20sum%20to%201.&amp;text=A%20transition%20matrix%20where%20the,stochastic%20(or%20left%20stochastic)."><em>row stochastic</em></a> Markov chain.</p>
</div>
</div>
